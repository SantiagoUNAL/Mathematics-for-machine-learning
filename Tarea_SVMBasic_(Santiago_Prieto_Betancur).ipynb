{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantiagoUNAL/Mathematics-for-machine-learning/blob/main/Tarea_SVMBasic_(Santiago_Prieto_Betancur).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-y8Kil2snGk"
      },
      "source": [
        "# Code Assigment 1\n",
        "\n",
        "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
        "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
        "\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
        "\n",
        "You should:\n",
        "\n",
        "1) Specify which Machine Learning problem are you solving.\n",
        "\n",
        "2) Provide a short summary of the features and the labels you are working on.\n",
        "\n",
        "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
        "\n",
        "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
        "\n",
        "5) Show some examples to illustrate that the method is working properly.\n",
        "\n",
        "6) Provide quantitative evidence for generalization using the provided dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyxJ_Juc6NsW"
      },
      "source": [
        "# Implementaci√≥n de SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5AOO-Ib6o_7U",
        "outputId": "e5c750e3-01b8-4ff4-a243-4512a5bc03c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "Optimized a step.\n",
            "Optimized a step.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        # La subclase llama los datos para utilizarlos\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        # El diccionario va a almacenar las parejas [w,b] etiquetadas por la norma de w (||w||) tales que satisfagan que yi(xi.w+b) >= 1\n",
        "        \n",
        "        opt_dict = {}\n",
        "\n",
        "        # El vector de vectores llamado transforms va a ser utilizado para evaluar todas las posibles direcciones que puede tomar un vector w dado. De esta manera, se\n",
        "        # examinan todas las posibles lineas determinadas por un vector de entrada w.\n",
        "        transforms = [[1,1],\n",
        "                      [-1,1],\n",
        "                      [-1,-1],\n",
        "                      [1,-1]]\n",
        "\n",
        "        # Se crea temporalmente un arreglo para buscar el n√∫mero m√°ximo entre todos los datos para usar este valor como un parametro que determine los rangos de operaci√≥n\n",
        "        # de todo el programa, as√≠ como para fijar cual va a ser el vector w inicial en el programa.\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "        \n",
        "        # Se determina cual es el valor m√°s peque√±o y m√°s grande de todos los datos y despues se vacia el arreglo \"all_data\" para no ocupar una gran cantidad de memoria.\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "        # En esta parte se establecen los pasos en los cuales se va a ir variando el vector w para el proceso de optimizaci√≥n.\n",
        "        # Inicialmente se dan pasos \"grandes\" hasta llegar a un punto en donde el vector w(i+1) supere al vector w(i) y despues se sigue la iteraci√≥n con los pasos m√°s peque√±os.\n",
        "\n",
        "        # Esta estrategia de usar pasos que varien el vector w es funcional porque el problema de minimizar la norma de w es un problema de optimizaci√≥n convexa en el cual se sabe\n",
        "        # que existe un m√≠nimo global de la funci√≥n\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "\n",
        "        # En esta parte se establecen las variables que definiran el rango donde variar√° el escalar b para determinar si cierta escogencia de w \n",
        "        # cumple que yi(xi.w+b) >= 1 para todos los datos.\n",
        "\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "\n",
        "        # Aqu√≠ no es necesario tener la misma precisi√≥n de la escogencia de b a diferencia de como se escoge w puesto que el valor de b no tiene muchas restricciones \n",
        "        # en la condici√≥n yi(xi.w+b) >= 1 lo cual permite ahorrar tiempo de computo. Sin embargo, se desea en la medida de lo posible que para los datos de entrenamiento\n",
        "        # los valores de b esten sujetos a la condici√≥n de frontera yi(xi.w+b) = 1\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10 #Este valor sera el que determinara todas las componentes del vector w inicial y en las iteraciones posteriores se ira cambiando\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            # Esta escogencia permite ahorrar tiempo de computo y es razonable puesto que el problema es de optimizaci√≥n convexa.\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                  # En esta parte del c√≥digo se evalua para cada w los posibles b que puedan satisfacer la restricci√≥n yi(xi.w+b) >= 1\n",
        "\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        # Aqu√≠ se evalua cada una de las posibles direcciones que puede tomar w\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                # En esta parte se determina si la escogencia particular de w y b satisfacen la restricci√≥n yi(xi.w+b) >= 1\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            # Una vez encontrada una pareja [w,b] para la cual se satisface la restricci√≥n yi(xi.w+b) >= 1, se procede a guardarla en un diccionario\n",
        "                            # etiquetado por la norma de w para despues poder hacer la busqueda y escoger el w con norma m√°s peque√±a\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                # En esta parte se determina cuando el programa llega al final de aplicar la serie de pasos mirando cuando la primera componente del vector w cambia de signo\n",
        "                # en este caso no importa que componente se mira, porque todas estan definidas igual.\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            # En esta √∫ltima parte se ordena el diccionario \"opt_dict\" de tal forma que la primera entrada sea el [w,b] donde w tiene la menor norma (min||w||)\n",
        "            # y apartir de ah√≠ escoger el nuevo w que va ir cambiando con pasos m√°s peque√±os y as√≠ ir mejorando la escogencia de w.\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "    # En esta parte se desarrolla la predicci√≥n para un conjunto de datos de prueba. Para hacer la clasificaci√≥n se mira la funci√≥n sign( x.w+b )\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "        \n",
        "# Este es el diccionario que contiene todos los datos de entrenamiento, clasificados por -1 si no tiene la caracteristica en estudio y 1 si tiene la caracteristica en estudio      \n",
        "data_dict = {-1:np.array([[1,7],\n",
        "                          [2,8],\n",
        "                          [3,8],]),\n",
        "             \n",
        "             1:np.array([[5,1],\n",
        "                         [6,-1],\n",
        "                         [7,3],])}\n",
        "\n",
        "# Aqu√≠ se llama a la clase Support_Vector_Machine() como svm1\n",
        "svm1 = Support_Vector_Machine()\n",
        "\n",
        "# Aqu√≠ se ejecuta la subclase de entrenamiento de la clase svm1 usando los datos de entrenamiento definidos en el diccionario \"data_dict\"\n",
        "svm1.fit(data_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmE-qAQOqwJ8",
        "outputId": "0e4a4e53-8b18-4beb-fb05-7ceb82056896"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "svm1.predict([7,3.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Respuestas de las preguntas"
      ],
      "metadata": {
        "id": "0Q3oIMGJ8CN7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxnSrTM3Q9HF"
      },
      "source": [
        "1) Este tipo de problema corresponde a aprendizaje de maquina supervisado puesto que los vectores de caracteristicas estan etiquetados en cada uno de los datasets en las clases 1 o -1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmfoBWPatpH"
      },
      "source": [
        "2) En el primer dataset (https://archive.ics.uci.edu/ml/datasets/banknote+authentication) se tiene una serie de datos que provienen de imagenes que fueron tomadas para la evaluaci√≥n de un procedimiento de autenticaci√≥n de billetes, es decir son datos tomados de especimenes de billetes genuinos y falsos. Para determinar el vector de caracteristicas se us√≥ como herramienta la transformada ond√≠cula (Wavelet Transform) que permite extraer caracteristicas de las imagenes tomadas de los distintos billetes.\n",
        "\n",
        "Para usar esta herramienta se hace uso de la transformada ond√≠cula integral definida como\n",
        "$$[W_{œà}f](a,b) = \\frac{1}{\\sqrt{|a|}} \\int_{-‚àû}^{‚àû}\\overline{œà\\left(\\frac{x-b}{a}\\right)}f(x)dx$$\n",
        "donde $œà \\in L^{2}(\\mathbb{R})$ es una funci√≥n llamada ond√≠cula ortonormal (orthonormal wavelet) que es usada para definir una base de Hilbert, que es un sistema completo ortonormal, para el espacio de Hilbert $L^{2}(\\mathbb{R})$ de las funciones cuadrado integrables.\n",
        "\n",
        "Las caracteristicas que definen cada una de las componentes del vector de caracteristicas son las siguientes:\n",
        "\n",
        "- Primera componente: Diferencia de la imagen transformada ond√≠cula (Wavelet Transformed image) - (continua)\n",
        "- Segunda componente: Asimetr√≠a de la imagen transformada ond√≠cula (Wavelet Transformed image) - (continua)\n",
        "- Tercera componente: Curtosis de la imagen transformada ond√≠cula (Wavelet Transformed image) - (continua)\n",
        "- Cuarta componente: Entrop√≠a de la imagen - (continua)\n",
        "- Quinta componente: Clase - (n√∫mero entero 0 si es falso o 1 si es real)\n",
        "\n",
        "\n",
        "En el segundo dataset (https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) se tiene una serie de datos que establecen la ocupaci√≥n de la tierra de fotos con sello de tiempo que fueron tomadas cada minuto. Estos datos experimentales fueron usados para clasificaci√≥n binaria (ocupaci√≥n de una habitaci√≥n).\n",
        "\n",
        "Las caracteristicas que definen cada una de las componentes del vector de caracteristicas son las siguientes:\n",
        "\n",
        "- Primera componente: Fecha y hora (a√±o-mes-d√≠a hora:minutos:segundos)\n",
        "- Segunda componente: Temperatura en Celsius\n",
        "- Tercera componente: Humedad relativa (Porcentaje - %)\n",
        "- Cuarta componente: Luz en Lux\n",
        "- Quinta componente: CO2 en ppm (Partes por mill√≥n)\n",
        "- Sexta componente: Tasa de humedad, cantidad derivada de la temperatura y la humedad relativa en $\\frac{[kg(vapor \\; de \\; agua)]}{[kg(aire)]}$\n",
        "- Septima componente: Ocupaci√≥n, 0 √≥ 1, 0 para no ocupado y 1 para el estado de ocupado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE87ZjA95r9M"
      },
      "source": [
        "3) a) Para ver que estos datasets originalmente tienen un alto grado de ser linealmente separables usaremos el algoritmo del perceptron. Para la primera base de datos (Banknote authentication Data Set) tenemos que el algoritmo del perceptron es el siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", sep=\",\", header=None)"
      ],
      "metadata": {
        "id": "cZb5Oe-yLaZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "clf = Perceptron(tol=1e-16, max_iter = 10000)\n",
        "\n",
        "clf.fit(df[[0,1,2,3]],df[4])\n",
        "\n",
        "y_pred = clf.predict(df[[0,1,2,3]])\n",
        "\n",
        "print(classification_report(df[4], y_pred, target_names = ['Clase Positiva (1)', 'Clase Negativa (-1)']))\n",
        "\n",
        "print(np.linalg.norm(df[4] - y_pred, ord = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6E-umZPLYBQ",
        "outputId": "d38a5326-9e46-43a3-d63b-d26b4477b7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            " Clase Positiva (1)       0.99      0.99      0.99       762\n",
            "Clase Negativa (-1)       0.98      0.99      0.99       610\n",
            "\n",
            "           accuracy                           0.99      1372\n",
            "          macro avg       0.99      0.99      0.99      1372\n",
            "       weighted avg       0.99      0.99      0.99      1372\n",
            "\n",
            "16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera, tenemos que es altamente probable que esta base de datos es linealmente separable puesto que el perceptron converge en un porcentaje muy alto.\n",
        "\n",
        "Para la segunda base de datos (Occupancy Detection Data Set) tenemos que el algoritmo del perceptron es el siguiente:"
      ],
      "metadata": {
        "id": "qe-7V2yIML1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "f1 = z.open(\"datatest.txt\")\n",
        "df1 = pd.read_csv(f1, sep =',')\n",
        "\n",
        "f2 = z.open(\"datatest2.txt\")\n",
        "df2 = pd.read_csv(f2, sep =',')\n",
        "\n",
        "\n",
        "f3 = z.open(\"datatraining.txt\")\n",
        "df3 = pd.read_csv(f3, sep =',')\n",
        "\n",
        "df4 = pd.concat([df1, df2, df3])"
      ],
      "metadata": {
        "id": "DYhsvWwcNLUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf2 = Perceptron(tol=1e-10, max_iter = 10000)\n",
        "\n",
        "X = df4[['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']]\n",
        "clf2.fit(X,df4['Occupancy'])\n",
        "\n",
        "y_pred = clf2.predict(X)\n",
        "\n",
        "print(classification_report(df4['Occupancy'], y_pred, target_names = ['clase1', 'clase2']))\n",
        "\n",
        "print(np.linalg.norm(df4['Occupancy'] - y_pred, ord = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2I_6VN7M8mt",
        "outputId": "49022ab7-ed00-45e7-d991-78b84aa99be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      clase1       1.00      0.95      0.98     15810\n",
            "      clase2       0.87      1.00      0.93      4750\n",
            "\n",
            "    accuracy                           0.96     20560\n",
            "   macro avg       0.93      0.98      0.95     20560\n",
            "weighted avg       0.97      0.96      0.97     20560\n",
            "\n",
            "732.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De la misma manera que el caso anterior tenemos que es altamente probable que esta base de datos es linealmente separable puesto que el perceptron converge en un porcentaje muy alto."
      ],
      "metadata": {
        "id": "0OsTNk7nNghB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) En general se puede conjeturar que estos datos no estan aleatoreamente escogidos puesto que en ambos casos, como se pudo evidenciar en los algoritmos anteriores, se tiene que es muy probable que exista una forma clara de separaci√≥n de los datos por lo que estos tienen reglas de asociaci√≥n entre ellos. Ademas estos datos provienen de"
      ],
      "metadata": {
        "id": "5Jy2w578No7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Estas bases de datos tienen un tama√±o suficiente para garantizar generalizaci√≥n como se vera m√°s adelante en la implementaci√≥n, puesto que estas bases de datos poseen muchos m√°s datos de entrenamiento que parametros a determinar en el SVM. Ademas dentro de la implemetaci√≥n se tiene que en el proceso de optimizaci√≥n al minimizar la norma del vector $\\vec{w}$ se tiene que se est√° maximizando la distancia de la frontera de decisi√≥n de los datos lo cual provee un mecanismo de generalizaci√≥n dentro del programa. "
      ],
      "metadata": {
        "id": "tDqARFzeOY5F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUUTj-Ii6Jpc"
      },
      "source": [
        "4) En esta parte vamos a dar una explicaci√≥n del c√≥mo y por qu√© el c√≥digo que implementa el SVM funciona. En la primera secci√≥n del c√≥digo lo que se hace es exportar los paquetes de Python necesarios para poder realizar las operaciones o uso de funciones necesarias para desarrollar la implementaci√≥n. Para esto se crea una clase llamada \"Support_Vector_Machine\" que en un principio ejecuta la clase gr√°ficando una figura vacia. Como se muestra a continuaci√≥n:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "e1XWRnCZ5s5t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAucONCk9B8z"
      },
      "source": [
        "La siguiente porci√≥n de la clase \"Support_Vector_Machine\" va a ser la encargada del entrenamiento de la m√°quina usando los datos de entrenamiento. En esta parte se definen los pasos que se toman para ir variando el vector $\\vec{w}$ y $b$, as√≠ como todas las posibles direcciones que puede tomar el vector $\\vec{w}$ definiendo en cada caso hiperplanos distintos. Tambien en esta parte se determina si el vector $\\vec{w}$ y el valor $b$ cumplen en cada caso la condici√≥n $y_{i}(w\\cdot x_{i} + b) \\geq 1$ para todos los vectores de caracteristicas $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ que se usan para entrenar al SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# train\n",
        "    def fit(self, data):\n",
        "        # La subclase llama los datos para utilizarlos\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        # El diccionario va a almacenar las parejas [w,b] etiquetadas por la norma de w (||w||) tales que satisfagan que yi(xi.w+b) >= 1\n",
        "        \n",
        "        opt_dict = {}\n",
        "\n",
        "        # El vector de vectores llamado transforms va a ser utilizado para evaluar todas las posibles direcciones que puede tomar un vector w dado. De esta manera, se\n",
        "        # examinan todas las posibles lineas determinadas por un vector de entrada w.\n",
        "        transforms = [[1,1],\n",
        "                      [-1,1],\n",
        "                      [-1,-1],\n",
        "                      [1,-1]]\n",
        "\n",
        "        # Se crea temporalmente un arreglo para buscar el n√∫mero m√°ximo entre todos los datos para usar este valor como un parametro que determine los rangos de operaci√≥n\n",
        "        # de todo el programa, as√≠ como para fijar cual va a ser el vector w inicial en el programa.\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "        \n",
        "        # Se determina cual es el valor m√°s peque√±o y m√°s grande de todos los datos y despues se vacia el arreglo \"all_data\" para no ocupar una gran cantidad de memoria.\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "        # En esta parte se establecen los pasos en los cuales se va a ir variando el vector w para el proceso de optimizaci√≥n.\n",
        "        # Inicialmente se dan pasos \"grandes\" hasta llegar a un punto en donde el vector w(i+1) supere al vector w(i) y despues se sigue la iteraci√≥n con los pasos m√°s peque√±os.\n",
        "\n",
        "        # Esta estrategia de usar pasos que varien el vector w es funcional porque el problema de minimizar la norma de w es un problema de optimizaci√≥n convexa en el cual se sabe\n",
        "        # que existe un m√≠nimo global de la funci√≥n\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "\n",
        "        # En esta parte se establecen las variables que definiran el rango donde variar√° el escalar b para determinar si cierta escogencia de w \n",
        "        # cumple que yi(xi.w+b) >= 1 para todos los datos.\n",
        "\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "\n",
        "        # Aqu√≠ no es necesario tener la misma precisi√≥n de la escogencia de b a diferencia de como se escoge w puesto que el valor de b no tiene muchas restricciones \n",
        "        # en la condici√≥n yi(xi.w+b) >= 1 lo cual permite ahorrar tiempo de computo. Sin embargo, se desea en la medida de lo posible que para los datos de entrenamiento\n",
        "        # los valores de b esten sujetos a la condici√≥n de frontera yi(xi.w+b) = 1\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10 #Este valor sera el que determinara todas las componentes del vector w inicial y en las iteraciones posteriores se ira cambiando\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            # Esta escogencia permite ahorrar tiempo de computo y es razonable puesto que el problema es de optimizaci√≥n convexa.\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                  # En esta parte del c√≥digo se evalua para cada w los posibles b que puedan satisfacer la restricci√≥n yi(xi.w+b) >= 1\n",
        "\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        # Aqu√≠ se evalua cada una de las posibles direcciones que puede tomar w\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                # En esta parte se determina si la escogencia particular de w y b satisfacen la restricci√≥n yi(xi.w+b) >= 1\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            # Una vez encontrada una pareja [w,b] para la cual se satisface la restricci√≥n yi(xi.w+b) >= 1, se procede a guardarla en un diccionario\n",
        "                            # etiquetado por la norma de w para despues poder hacer la busqueda y escoger el w con norma m√°s peque√±a\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                # En esta parte se determina cuando el programa llega al final de aplicar la serie de pasos mirando cuando la primera componente del vector w cambia de signo\n",
        "                # en este caso no importa que componente se mira, porque todas estan definidas igual.\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            # En esta √∫ltima parte se ordena el diccionario \"opt_dict\" de tal forma que la primera entrada sea el [w,b] donde w tiene la menor norma (min||w||)\n",
        "            # y apartir de ah√≠ escoger el nuevo w que va ir cambiando con pasos m√°s peque√±os y as√≠ ir mejorando la escogencia de w.\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mPwayTHK41O0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQgJ5cDO3dQ"
      },
      "source": [
        "La siguiente parte corresponde a la subclase que permite realizar la predicci√≥n de nuevos datos. En particular, de aquellos que no tienen una etiqueta definida."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# En esta parte se desarrolla la predicci√≥n para un conjunto de datos de prueba. Para hacer la clasificaci√≥n se mira la funci√≥n sign( x.w+b )\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ANxAN0a37WIq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPDbjgLvPA25"
      },
      "source": [
        "Finalmente, el c√≥digo almacena los datos de entrenamiento en un diccionario y llama a la clase \"Support_Vector_Machine\" por svm1 para entrenarlo con los datos definidos en el diccionario."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Este es el diccionario que contiene todos los datos de entrenamiento, clasificados por -1 si no tiene la caracteristica en estudio y 1 si tiene la caracteristica en estudio      \n",
        "data_dict = {-1:np.array([[1,7],\n",
        "                          [2,8],\n",
        "                          [3,8],]),\n",
        "             \n",
        "             1:np.array([[5,1],\n",
        "                         [6,-1],\n",
        "                         [7,3],])}\n",
        "\n",
        "# Aqu√≠ se llama a la clase Support_Vector_Machine() como svm1\n",
        "svm1 = Support_Vector_Machine()\n",
        "\n",
        "# Aqu√≠ se ejecuta la subclase de entrenamiento de la clase svm1 usando los datos de entrenamiento definidos en el diccionario \"data_dict\"\n",
        "svm1.fit(data_dict)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JszDIqZx7j1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n resolveremos las preguntas 5) y 6) en los distintos datasaets en donde mostraremos como la implementaci√≥n del programa SVM funciona bajo ciertas restricciones y como este a su vez permite dar evidencia cuantitativa de que los resultados generalizan el problema de clasificaci√≥n trabajado por la SVM."
      ],
      "metadata": {
        "id": "_T4JSjhRDW9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementaci√≥n para la primera base de datos (Banknote authentication Data Set)"
      ],
      "metadata": {
        "id": "TrwyQeqkYD6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder implementar el c√≥digo se introdujo una tolerancia para la cual la SVM puede fallar en algunos datos. Sin embargo, como se vera a continuaci√≥n esto sigue proveyendo buenos resultados."
      ],
      "metadata": {
        "id": "64sqnVc9zXLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBJM4zO-5aSk"
      },
      "outputs": [],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine_1:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        opt_dict = {}\n",
        "\n",
        "        transforms = [[1,1,1,1],\n",
        "                      [1,1,1,-1],\n",
        "                      [1,1,-1,1],\n",
        "                      [1,1,-1,-1],\n",
        "                      [1,-1,1,1],\n",
        "                      [1,-1,1,-1],\n",
        "                      [1,-1,-1,1],\n",
        "                      [-1,1,1,1],\n",
        "                      [-1,1,1,-1],\n",
        "                      [-1,1,-1,1],\n",
        "                      [-1,1,-1,-1],\n",
        "                      [-1,-1,1,1],\n",
        "                      [-1,-1,1,-1],\n",
        "                      [-1,-1,-1,1],\n",
        "                      [-1,-1,-1,-1],\n",
        "                      [1,-1,-1,-1]]\n",
        "\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "        all_data = []\n",
        "\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10\n",
        "\n",
        "        tolerance = 0.03\n",
        "        total_data = self.data[1].shape[0] + self.data[-1].shape[0]\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum,latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        misses = 0\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    misses = misses + 1\n",
        "                                    # found_option = False\n",
        "                        \n",
        "                        rate = misses/total_data\n",
        "                        if rate > tolerance:\n",
        "                          found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            print(norms)\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "\n",
        "    def predict_vector(self, X):\n",
        "        #X is a set of data, predict_vector is an array with the predictions\n",
        "        len_data = X.shape[0]\n",
        "        y = np.zeros(len_data)\n",
        "        for i in range(len_data):\n",
        "          y[i] = self.predict(X[i])\n",
        "        return np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte se importan los datos para poder crear el diccionario que entrenara la SVM. Para esto se toma una muestra aleatoria del $80 \\%$ de los datos y se toma el restante $20 \\%$ de los datos para realizar predicciones y comprobar que la SVM generaliza los datos."
      ],
      "metadata": {
        "id": "j4grSg26-Rtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", sep=\",\", header=None)\n",
        "\n",
        "percentage_sample = 0.8\n",
        "sample_size = df.shape[0]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df[[0,1,2,3]],\n",
        "    df[4],\n",
        "    test_size=0.2,\n",
        "    stratify=df[4],\n",
        "    random_state=24,\n",
        ")\n",
        "\n",
        "\n",
        "positive_class = x_train.loc[df[4] == 1]\n",
        "# positive_class = positive_class.sample(int(sample_size*percentage_sample))\n",
        "positive_class = positive_class[[0,1,2,3]].to_numpy()\n",
        "negative_class = x_train.loc[df[4] == 0]\n",
        "# negative_class = negative_class.sample(int(sample_size*percentage_sample))\n",
        "negative_class = negative_class[[0,1,2,3]].to_numpy()\n",
        "\n",
        "\n",
        "training_data_dict = {1 : positive_class,\n",
        "        -1: negative_class}"
      ],
      "metadata": {
        "id": "_BVs4NNzrrEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_1 = Support_Vector_Machine_1()\n",
        "svm_1.fit(training_data_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "dKHpnb03gGsk",
        "outputId": "99216162-6618-4c69-813d-8a7a7db2f4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "[107.56439999999984, 121.90631999999985]\n",
            "Optimized a step.\n",
            "[100.39343999999984, 107.56439999999984, 121.90631999999985]\n",
            "Optimized a step.\n",
            "[99.67634399999984, 100.39343999999984, 107.56439999999984, 121.90631999999985]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte se evalua la precisi√≥n de la SVM prediciendo los datos de prueba y as√≠ poder medir el desempe√±o del programa generalizando los datos."
      ],
      "metadata": {
        "id": "WJWrSY5T--B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svm_1.predict_vector(x_test.to_numpy())\n",
        "y_test1 = np.where(y_test < 1, -1, 1)\n",
        "print(classification_report(y_test1, y_pred, target_names = ['Clase Positiva (1)', 'Clase Negativa (-1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXmgg9FIiL2y",
        "outputId": "b87be6c6-f455-4132-ea04-1bd6be967141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            " Clase Positiva (1)       0.99      0.93      0.96       153\n",
            "Clase Negativa (-1)       0.92      0.98      0.95       122\n",
            "\n",
            "           accuracy                           0.96       275\n",
            "          macro avg       0.95      0.96      0.96       275\n",
            "       weighted avg       0.96      0.96      0.96       275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqu√≠ podemos ver como la SVM logra tener una precisi√≥n general del $95 \\% $ prediciendo los datos de prueba, mostrandonos que este data set es en un alto porcentaje linealmente separable salvo algunos outliers. Sin embargo, cabe resaltar que hay un mayor sesgo prediciendo aquellos datos de la clase negativa lo cual puede ser producto de que la frontera de decisi√≥n no esta muy bien definida. Sin embargo, el SVM logra generalizar el comportamiento de los datos al $20 \\%$ de los datos que no se utilizaron en el entrenamiento de la m√°quina."
      ],
      "metadata": {
        "id": "PGIaean3_RbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementaci√≥n para la segunda base de datos (Occupancy Detection Data Set)"
      ],
      "metadata": {
        "id": "KcAMVM6CYdRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder implementar el c√≥digo se introdujo una tolerancia para la cual la SVM puede fallar en algunos datos. Sin embargo, como se vera a continuaci√≥n esto sigue proveyendo buenos resultados."
      ],
      "metadata": {
        "id": "_otepJwI8Aep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3vgCTrZo8hm"
      },
      "outputs": [],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests, zipfile, io\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine_2:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        opt_dict = {}\n",
        "\n",
        "        transforms = [[1,1,1,1,1],\n",
        "                      [1,1,1,1,-1],\n",
        "                      [1,1,1,-1,1],\n",
        "                      [1,1,1,-1,-1],\n",
        "                      [1,1,-1,1,1],\n",
        "                      [1,1,-1,1,-1],\n",
        "                      [1,1,-1,-1,1],\n",
        "                      [1,1,-1,-1,-1],\n",
        "                      [1,-1,1,1,1],\n",
        "                      [1,-1,1,1,-1],\n",
        "                      [1,-1,1,-1,1],\n",
        "                      [1,-1,1,-1,-1],\n",
        "                      [1,-1,-1,1,1],\n",
        "                      [1,-1,-1,1,-1],\n",
        "                      [1,-1,-1,-1,1],\n",
        "                      [-1,1,1,1,1],\n",
        "                      [-1,1,1,1,-1],\n",
        "                      [-1,1,1,-1,1],\n",
        "                      [-1,1,1,-1,-1],\n",
        "                      [-1,1,-1,1,1],\n",
        "                      [-1,1,-1,1,-1],\n",
        "                      [-1,1,-1,-1,1],\n",
        "                      [-1,1,-1,-1,-1],\n",
        "                      [-1,-1,1,1,1],\n",
        "                      [-1,-1,1,1,-1],\n",
        "                      [-1,-1,1,-1,1],\n",
        "                      [-1,-1,1,-1,-1],\n",
        "                      [-1,-1,-1,1,1],\n",
        "                      [-1,-1,-1,1,-1],\n",
        "                      [-1,-1,-1,-1,1],\n",
        "                      [-1,-1,-1,-1,-1],\n",
        "                      [1,-1,-1,-1,-1]]\n",
        "\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01]\n",
        "                      # point of expense:\n",
        "                      # self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10\n",
        "\n",
        "        tolerance = 0.2\n",
        "        total_data = self.data[1].shape[0] + self.data[-1].shape[0]\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum,latest_optimum,latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "\n",
        "                        misses = 0\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    misses = misses + 1\n",
        "                                    rate = misses/total_data\n",
        "                                    if rate > tolerance:\n",
        "                                        found_option = False\n",
        "                                        break\n",
        "                                      \n",
        "                                    # found_option = False\n",
        "                        \n",
        "                        # rate = misses/total_data\n",
        "                        # if rate > tolerance:\n",
        "                          # found_option = False\n",
        "\n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            print(norms)\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "    \n",
        "    def predict_vector(self, X):\n",
        "        #X is a set of data, predict_vector is an array with the predictions\n",
        "        len_data = X.shape[0]\n",
        "        y = np.zeros(len_data)\n",
        "        for i in range(len_data):\n",
        "          y[i] = self.predict(X[i])\n",
        "        return np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n se extraen los distintos archivos de la carpeta Zip del data set de \"Detecci√≥n de ocupaci√≥n\". Inicialmente se extrae el archivo \"datatraining.txt\" para poder formar el diccionario que tendra los datos de entrenamiento."
      ],
      "metadata": {
        "id": "uMm5jKRm2BSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "f1 = z.open(\"datatraining.txt\")\n",
        "df1 = pd.read_csv(f1, sep=\",\")\n",
        "\n",
        "percentage_sample = 0.01\n",
        "sample_size = df1.shape[0]\n",
        "\n",
        "positive = df1.loc[df1[\"Occupancy\"] == 1]\n",
        "negative = df1.loc[df1[\"Occupancy\"] == 0]\n",
        "\n",
        "training_positive = positive.iloc[:, 1:6]\n",
        "# positive_arr = training_positive.to_numpy()\n",
        "\n",
        "training_positive_2 = training_positive.sample(int(training_positive.shape[0]*percentage_sample))\n",
        "positive_arr_2 = training_positive_2.to_numpy()\n",
        "\n",
        "training_negative = negative.iloc[:, 1:6]\n",
        "# negative_arr = training_negative.to_numpy()\n",
        "\n",
        "training_negative_2 = training_negative.sample(int(training_negative.shape[0]*percentage_sample))\n",
        "negative_arr_2 = training_negative_2.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "training_data_dict_2 = {-1:negative_arr_2,\n",
        "                      1:positive_arr_2}"
      ],
      "metadata": {
        "id": "l7zq_Osiqpju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_2 = Support_Vector_Machine_2()\n",
        "svm_2.fit(training_data_dict_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "ttikPJZ7seOd",
        "outputId": "5c3e15f3-d7b4-475a-953c-584045e424c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "[438.7724388848927, 438.7724388848998, 877.5448777697961, 1316.3173166546924, 1755.0897555395886]\n",
            "Optimized a step.\n",
            "[43.877243888486454, 87.75448777697608, 131.6317316654657, 175.50897555395534, 219.38621944244497, 263.2634633309346, 307.1407072194242, 351.01795110791386, 394.89519499640346, 438.7724388848927, 438.7724388848931, 438.7724388848998, 482.6496827733827, 526.5269266618724, 570.404170550362, 614.2814144388516, 658.1586583273413, 702.0359022158309, 745.9131461043205, 789.7903899928101, 833.6676338812997, 877.5448777697894, 877.5448777697961, 921.422121658279, 965.2993655467687, 1009.1766094352582, 1053.053853323748, 1096.9310972122375, 1140.8083411007271, 1184.6855849892167, 1228.5628288777064, 1272.4400727661957, 1316.3173166546853, 1316.3173166546924, 1755.0897555395886]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la parte siguiente se exportan los archivos que se van a utilizar para hacer las predicciones y poder medir posteriormente el grado de precisi√≥n del SVM para este data set de \"Detecci√≥n de ocupaci√≥n\"."
      ],
      "metadata": {
        "id": "Ee-3ovV_2wHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f2 = z.open(\"datatest.txt\")\n",
        "df2 = pd.read_csv(f2, sep =',')\n",
        "\n",
        "f3 = z.open(\"datatest2.txt\")\n",
        "df3 = pd.read_csv(f3, sep =',')\n",
        "\n",
        "df4 = pd.concat([df2, df3])\n",
        "\n",
        "positive = df4.loc[df4[\"Occupancy\"] == 1]\n",
        "negative = df4.loc[df4[\"Occupancy\"] == 0]\n",
        "\n",
        "predict_positive = positive.iloc[:, 1:6]\n",
        "positive_arr = predict_positive.to_numpy()\n",
        "\n",
        "predict_negative = negative.iloc[:, 1:6]\n",
        "negative_arr = predict_negative.to_numpy()\n",
        "\n",
        "predict = df4.iloc[:, 1:6]\n",
        "predict_array = predict.to_numpy()\n",
        "\n",
        "y_test = df4.iloc[:,6]\n",
        "y_test_arr = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "irbvSpgIuVj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_pred2 = svm_2.predict_vector(predict_array)\n",
        "y_test2 = np.where(y_test_arr < 1, -1, 1)\n",
        "print(classification_report(y_test2, y_pred2, target_names = ['Clase Ocupado (1)', 'Clase No Ocupado (-1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL5G1Y-Rsqr1",
        "outputId": "7186c8b4-45b6-4423-d935-d49f81790c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Clase Ocupado (1)       0.84      0.97      0.90      9396\n",
            "Clase No Ocupado (-1)       0.82      0.41      0.54      3021\n",
            "\n",
            "             accuracy                           0.83     12417\n",
            "            macro avg       0.83      0.69      0.72     12417\n",
            "         weighted avg       0.83      0.83      0.81     12417\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqu√≠ podemos ver como la SVM logra tener una precisi√≥n general del $83 \\% $ prediciendo los datos de prueba, mostrandonos que este data set es en un alto porcentaje linealmente separable salvo algunos outliers. Sin embargo, cabe resaltar que hay un mayor sesgo prediciendo aquellos datos de la clase negativa lo cual puede ser producto de que la frontera de decisi√≥n no esta muy bien definida. Aqu√≠ hay que hacer la salvedad de que esta precisi√≥n corresponde a tan solo tomar el $1 \\%$ de los datos en \"datatraining.txt\" y usarlos para entrenar el SVM. Sin embargo, a pesar de la baja cantidad de datos en el entrenamiento podemos apreciar un gran desempe√±o del programa prediciendo los nuevos datos."
      ],
      "metadata": {
        "id": "JzXi3iMqD3qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otra parte, si utilizamos la libreria scikit-learn para entrenar estos datos con la implementaci√≥n del SVM de dicha libreria podemos observar que la separaci√≥n de los datos por un hiperplano tiene un gran porcentaje de precisi√≥n, como se puede observar a continuaci√≥n:"
      ],
      "metadata": {
        "id": "Zp_eSC-M2gbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "X = df1.iloc[:, 1:6].to_numpy()\n",
        "y = df1.iloc[:, 6].to_numpy()\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "H5EYp8eXy-6u",
        "outputId": "0cfe8598-a4b0-42ca-cafa-fabb66aa708b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(predict_array)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcGkET-dzXyk",
        "outputId": "aa17b00a-d6e2-4184-beb9-c040371d9320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test_arr, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPyPBVsu00JV",
        "outputId": "4b966224-0ab8-4048-a43f-e34482409476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.989530482403157\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}