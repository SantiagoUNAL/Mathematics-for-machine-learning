{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantiagoUNAL/Mathematics-for-machine-learning/blob/main/Tarea_SVMBasic_(Santiago_Prieto_Betancur).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-y8Kil2snGk"
      },
      "source": [
        "# Code Assigment 1\n",
        "\n",
        "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
        "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
        "\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
        "\n",
        "You should:\n",
        "\n",
        "1) Specify which Machine Learning problem are you solving.\n",
        "\n",
        "2) Provide a short summary of the features and the labels you are working on.\n",
        "\n",
        "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
        "\n",
        "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
        "\n",
        "5) Show some examples to illustrate that the method is working properly.\n",
        "\n",
        "6) Provide quantitative evidence for generalization using the provided dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyxJ_Juc6NsW"
      },
      "source": [
        "# Implementación de SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5AOO-Ib6o_7U",
        "outputId": "e5c750e3-01b8-4ff4-a243-4512a5bc03c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "Optimized a step.\n",
            "Optimized a step.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        # La subclase llama los datos para utilizarlos\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        # El diccionario va a almacenar las parejas [w,b] etiquetadas por la norma de w (||w||) tales que satisfagan que yi(xi.w+b) >= 1\n",
        "        \n",
        "        opt_dict = {}\n",
        "\n",
        "        # El vector de vectores llamado transforms va a ser utilizado para evaluar todas las posibles direcciones que puede tomar un vector w dado. De esta manera, se\n",
        "        # examinan todas las posibles lineas determinadas por un vector de entrada w.\n",
        "        transforms = [[1,1],\n",
        "                      [-1,1],\n",
        "                      [-1,-1],\n",
        "                      [1,-1]]\n",
        "\n",
        "        # Se crea temporalmente un arreglo para buscar el número máximo entre todos los datos para usar este valor como un parametro que determine los rangos de operación\n",
        "        # de todo el programa, así como para fijar cual va a ser el vector w inicial en el programa.\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "        \n",
        "        # Se determina cual es el valor más pequeño y más grande de todos los datos y despues se vacia el arreglo \"all_data\" para no ocupar una gran cantidad de memoria.\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "        # En esta parte se establecen los pasos en los cuales se va a ir variando el vector w para el proceso de optimización.\n",
        "        # Inicialmente se dan pasos \"grandes\" hasta llegar a un punto en donde el vector w(i+1) supere al vector w(i) y despues se sigue la iteración con los pasos más pequeños.\n",
        "\n",
        "        # Esta estrategia de usar pasos que varien el vector w es funcional porque el problema de minimizar la norma de w es un problema de optimización convexa en el cual se sabe\n",
        "        # que existe un mínimo global de la función\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "\n",
        "        # En esta parte se establecen las variables que definiran el rango donde variará el escalar b para determinar si cierta escogencia de w \n",
        "        # cumple que yi(xi.w+b) >= 1 para todos los datos.\n",
        "\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "\n",
        "        # Aquí no es necesario tener la misma precisión de la escogencia de b a diferencia de como se escoge w puesto que el valor de b no tiene muchas restricciones \n",
        "        # en la condición yi(xi.w+b) >= 1 lo cual permite ahorrar tiempo de computo. Sin embargo, se desea en la medida de lo posible que para los datos de entrenamiento\n",
        "        # los valores de b esten sujetos a la condición de frontera yi(xi.w+b) = 1\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10 #Este valor sera el que determinara todas las componentes del vector w inicial y en las iteraciones posteriores se ira cambiando\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            # Esta escogencia permite ahorrar tiempo de computo y es razonable puesto que el problema es de optimización convexa.\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                  # En esta parte del código se evalua para cada w los posibles b que puedan satisfacer la restricción yi(xi.w+b) >= 1\n",
        "\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        # Aquí se evalua cada una de las posibles direcciones que puede tomar w\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                # En esta parte se determina si la escogencia particular de w y b satisfacen la restricción yi(xi.w+b) >= 1\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            # Una vez encontrada una pareja [w,b] para la cual se satisface la restricción yi(xi.w+b) >= 1, se procede a guardarla en un diccionario\n",
        "                            # etiquetado por la norma de w para despues poder hacer la busqueda y escoger el w con norma más pequeña\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                # En esta parte se determina cuando el programa llega al final de aplicar la serie de pasos mirando cuando la primera componente del vector w cambia de signo\n",
        "                # en este caso no importa que componente se mira, porque todas estan definidas igual.\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            # En esta última parte se ordena el diccionario \"opt_dict\" de tal forma que la primera entrada sea el [w,b] donde w tiene la menor norma (min||w||)\n",
        "            # y apartir de ahí escoger el nuevo w que va ir cambiando con pasos más pequeños y así ir mejorando la escogencia de w.\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "    # En esta parte se desarrolla la predicción para un conjunto de datos de prueba. Para hacer la clasificación se mira la función sign( x.w+b )\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "        \n",
        "# Este es el diccionario que contiene todos los datos de entrenamiento, clasificados por -1 si no tiene la caracteristica en estudio y 1 si tiene la caracteristica en estudio      \n",
        "data_dict = {-1:np.array([[1,7],\n",
        "                          [2,8],\n",
        "                          [3,8],]),\n",
        "             \n",
        "             1:np.array([[5,1],\n",
        "                         [6,-1],\n",
        "                         [7,3],])}\n",
        "\n",
        "# Aquí se llama a la clase Support_Vector_Machine() como svm1\n",
        "svm1 = Support_Vector_Machine()\n",
        "\n",
        "# Aquí se ejecuta la subclase de entrenamiento de la clase svm1 usando los datos de entrenamiento definidos en el diccionario \"data_dict\"\n",
        "svm1.fit(data_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmE-qAQOqwJ8",
        "outputId": "0e4a4e53-8b18-4beb-fb05-7ceb82056896"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "svm1.predict([7,3.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Respuestas de las preguntas"
      ],
      "metadata": {
        "id": "0Q3oIMGJ8CN7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxnSrTM3Q9HF"
      },
      "source": [
        "1) Este tipo de problema corresponde a aprendizaje de maquina supervisado puesto que los vectores de caracteristicas estan etiquetados en cada uno de los datasets en las clases 1 o -1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmfoBWPatpH"
      },
      "source": [
        "2) En el primer dataset (https://archive.ics.uci.edu/ml/datasets/banknote+authentication) se tiene una serie de datos que provienen de imagenes que fueron tomadas para la evaluación de un procedimiento de autenticación de billetes, es decir son datos tomados de especimenes de billetes genuinos y falsos. Para determinar el vector de caracteristicas se usó como herramienta la transformada ondícula (Wavelet Transform) que permite extraer caracteristicas de las imagenes tomadas de los distintos billetes.\n",
        "\n",
        "Para usar esta herramienta se hace uso de la transformada ondícula integral definida como\n",
        "$$[W_{ψ}f](a,b) = \\frac{1}{\\sqrt{|a|}} \\int_{-∞}^{∞}\\overline{ψ\\left(\\frac{x-b}{a}\\right)}f(x)dx$$\n",
        "donde $ψ \\in L^{2}(\\mathbb{R})$ es una función llamada ondícula ortonormal (orthonormal wavelet) que es usada para definir una base de Hilbert, que es un sistema completo ortonormal, para el espacio de Hilbert $L^{2}(\\mathbb{R})$ de las funciones cuadrado integrables.\n",
        "\n",
        "Las caracteristicas que definen cada una de las componentes del vector de caracteristicas son las siguientes:\n",
        "\n",
        "- Primera componente: Diferencia de la imagen transformada ondícula (Wavelet Transformed image) - (continua)\n",
        "- Segunda componente: Asimetría de la imagen transformada ondícula (Wavelet Transformed image) - (continua)\n",
        "- Tercera componente: Curtosis de la imagen transformada ondícula (Wavelet Transformed image) - (continua)\n",
        "- Cuarta componente: Entropía de la imagen - (continua)\n",
        "- Quinta componente: Clase - (número entero 0 si es falso o 1 si es real)\n",
        "\n",
        "\n",
        "En el segundo dataset (https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) se tiene una serie de datos que establecen la ocupación de la tierra de fotos con sello de tiempo que fueron tomadas cada minuto. Estos datos experimentales fueron usados para clasificación binaria (ocupación de una habitación).\n",
        "\n",
        "Las caracteristicas que definen cada una de las componentes del vector de caracteristicas son las siguientes:\n",
        "\n",
        "- Primera componente: Fecha y hora (año-mes-día hora:minutos:segundos)\n",
        "- Segunda componente: Temperatura en Celsius\n",
        "- Tercera componente: Humedad relativa (Porcentaje - %)\n",
        "- Cuarta componente: Luz en Lux\n",
        "- Quinta componente: CO2 en ppm (Partes por millón)\n",
        "- Sexta componente: Tasa de humedad, cantidad derivada de la temperatura y la humedad relativa en $\\frac{[kg(vapor \\; de \\; agua)]}{[kg(aire)]}$\n",
        "- Septima componente: Ocupación, 0 ó 1, 0 para no ocupado y 1 para el estado de ocupado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE87ZjA95r9M"
      },
      "source": [
        "3) a) Para ver que estos datasets originalmente tienen un alto grado de ser linealmente separables usaremos el algoritmo del perceptron. Para la primera base de datos (Banknote authentication Data Set) tenemos que el algoritmo del perceptron es el siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", sep=\",\", header=None)"
      ],
      "metadata": {
        "id": "cZb5Oe-yLaZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "clf = Perceptron(tol=1e-16, max_iter = 10000)\n",
        "\n",
        "clf.fit(df[[0,1,2,3]],df[4])\n",
        "\n",
        "y_pred = clf.predict(df[[0,1,2,3]])\n",
        "\n",
        "print(classification_report(df[4], y_pred, target_names = ['Clase Positiva (1)', 'Clase Negativa (-1)']))\n",
        "\n",
        "print(np.linalg.norm(df[4] - y_pred, ord = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6E-umZPLYBQ",
        "outputId": "d38a5326-9e46-43a3-d63b-d26b4477b7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            " Clase Positiva (1)       0.99      0.99      0.99       762\n",
            "Clase Negativa (-1)       0.98      0.99      0.99       610\n",
            "\n",
            "           accuracy                           0.99      1372\n",
            "          macro avg       0.99      0.99      0.99      1372\n",
            "       weighted avg       0.99      0.99      0.99      1372\n",
            "\n",
            "16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera, tenemos que es altamente probable que esta base de datos es linealmente separable puesto que el perceptron converge en un porcentaje muy alto.\n",
        "\n",
        "Para la segunda base de datos (Occupancy Detection Data Set) tenemos que el algoritmo del perceptron es el siguiente:"
      ],
      "metadata": {
        "id": "qe-7V2yIML1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "f1 = z.open(\"datatest.txt\")\n",
        "df1 = pd.read_csv(f1, sep =',')\n",
        "\n",
        "f2 = z.open(\"datatest2.txt\")\n",
        "df2 = pd.read_csv(f2, sep =',')\n",
        "\n",
        "\n",
        "f3 = z.open(\"datatraining.txt\")\n",
        "df3 = pd.read_csv(f3, sep =',')\n",
        "\n",
        "df4 = pd.concat([df1, df2, df3])"
      ],
      "metadata": {
        "id": "DYhsvWwcNLUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf2 = Perceptron(tol=1e-10, max_iter = 10000)\n",
        "\n",
        "X = df4[['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']]\n",
        "clf2.fit(X,df4['Occupancy'])\n",
        "\n",
        "y_pred = clf2.predict(X)\n",
        "\n",
        "print(classification_report(df4['Occupancy'], y_pred, target_names = ['clase1', 'clase2']))\n",
        "\n",
        "print(np.linalg.norm(df4['Occupancy'] - y_pred, ord = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2I_6VN7M8mt",
        "outputId": "49022ab7-ed00-45e7-d991-78b84aa99be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      clase1       1.00      0.95      0.98     15810\n",
            "      clase2       0.87      1.00      0.93      4750\n",
            "\n",
            "    accuracy                           0.96     20560\n",
            "   macro avg       0.93      0.98      0.95     20560\n",
            "weighted avg       0.97      0.96      0.97     20560\n",
            "\n",
            "732.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De la misma manera que el caso anterior tenemos que es altamente probable que esta base de datos es linealmente separable puesto que el perceptron converge en un porcentaje muy alto."
      ],
      "metadata": {
        "id": "0OsTNk7nNghB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) En general se puede conjeturar que estos datos no estan aleatoreamente escogidos puesto que en ambos casos, como se pudo evidenciar en los algoritmos anteriores, se tiene que es muy probable que exista una forma clara de separación de los datos por lo que estos tienen reglas de asociación entre ellos. Ademas estos datos provienen de"
      ],
      "metadata": {
        "id": "5Jy2w578No7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Estas bases de datos tienen un tamaño suficiente para garantizar generalización como se vera más adelante en la implementación, puesto que estas bases de datos poseen muchos más datos de entrenamiento que parametros a determinar en el SVM. Ademas dentro de la implemetación se tiene que en el proceso de optimización al minimizar la norma del vector $\\vec{w}$ se tiene que se está maximizando la distancia de la frontera de decisión de los datos lo cual provee un mecanismo de generalización dentro del programa. "
      ],
      "metadata": {
        "id": "tDqARFzeOY5F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUUTj-Ii6Jpc"
      },
      "source": [
        "4) En esta parte vamos a dar una explicación del cómo y por qué el código que implementa el SVM funciona. En la primera sección del código lo que se hace es exportar los paquetes de Python necesarios para poder realizar las operaciones o uso de funciones necesarias para desarrollar la implementación. Para esto se crea una clase llamada \"Support_Vector_Machine\" que en un principio ejecuta la clase gráficando una figura vacia. Como se muestra a continuación:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "e1XWRnCZ5s5t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAucONCk9B8z"
      },
      "source": [
        "La siguiente porción de la clase \"Support_Vector_Machine\" va a ser la encargada del entrenamiento de la máquina usando los datos de entrenamiento. En esta parte se definen los pasos que se toman para ir variando el vector $\\vec{w}$ y $b$, así como todas las posibles direcciones que puede tomar el vector $\\vec{w}$ definiendo en cada caso hiperplanos distintos. Tambien en esta parte se determina si el vector $\\vec{w}$ y el valor $b$ cumplen en cada caso la condición $y_{i}(w\\cdot x_{i} + b) \\geq 1$ para todos los vectores de caracteristicas $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ que se usan para entrenar al SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# train\n",
        "    def fit(self, data):\n",
        "        # La subclase llama los datos para utilizarlos\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        # El diccionario va a almacenar las parejas [w,b] etiquetadas por la norma de w (||w||) tales que satisfagan que yi(xi.w+b) >= 1\n",
        "        \n",
        "        opt_dict = {}\n",
        "\n",
        "        # El vector de vectores llamado transforms va a ser utilizado para evaluar todas las posibles direcciones que puede tomar un vector w dado. De esta manera, se\n",
        "        # examinan todas las posibles lineas determinadas por un vector de entrada w.\n",
        "        transforms = [[1,1],\n",
        "                      [-1,1],\n",
        "                      [-1,-1],\n",
        "                      [1,-1]]\n",
        "\n",
        "        # Se crea temporalmente un arreglo para buscar el número máximo entre todos los datos para usar este valor como un parametro que determine los rangos de operación\n",
        "        # de todo el programa, así como para fijar cual va a ser el vector w inicial en el programa.\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "        \n",
        "        # Se determina cual es el valor más pequeño y más grande de todos los datos y despues se vacia el arreglo \"all_data\" para no ocupar una gran cantidad de memoria.\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "        # En esta parte se establecen los pasos en los cuales se va a ir variando el vector w para el proceso de optimización.\n",
        "        # Inicialmente se dan pasos \"grandes\" hasta llegar a un punto en donde el vector w(i+1) supere al vector w(i) y despues se sigue la iteración con los pasos más pequeños.\n",
        "\n",
        "        # Esta estrategia de usar pasos que varien el vector w es funcional porque el problema de minimizar la norma de w es un problema de optimización convexa en el cual se sabe\n",
        "        # que existe un mínimo global de la función\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "\n",
        "        # En esta parte se establecen las variables que definiran el rango donde variará el escalar b para determinar si cierta escogencia de w \n",
        "        # cumple que yi(xi.w+b) >= 1 para todos los datos.\n",
        "\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "\n",
        "        # Aquí no es necesario tener la misma precisión de la escogencia de b a diferencia de como se escoge w puesto que el valor de b no tiene muchas restricciones \n",
        "        # en la condición yi(xi.w+b) >= 1 lo cual permite ahorrar tiempo de computo. Sin embargo, se desea en la medida de lo posible que para los datos de entrenamiento\n",
        "        # los valores de b esten sujetos a la condición de frontera yi(xi.w+b) = 1\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10 #Este valor sera el que determinara todas las componentes del vector w inicial y en las iteraciones posteriores se ira cambiando\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            # Esta escogencia permite ahorrar tiempo de computo y es razonable puesto que el problema es de optimización convexa.\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                  # En esta parte del código se evalua para cada w los posibles b que puedan satisfacer la restricción yi(xi.w+b) >= 1\n",
        "\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        # Aquí se evalua cada una de las posibles direcciones que puede tomar w\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                # En esta parte se determina si la escogencia particular de w y b satisfacen la restricción yi(xi.w+b) >= 1\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            # Una vez encontrada una pareja [w,b] para la cual se satisface la restricción yi(xi.w+b) >= 1, se procede a guardarla en un diccionario\n",
        "                            # etiquetado por la norma de w para despues poder hacer la busqueda y escoger el w con norma más pequeña\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                # En esta parte se determina cuando el programa llega al final de aplicar la serie de pasos mirando cuando la primera componente del vector w cambia de signo\n",
        "                # en este caso no importa que componente se mira, porque todas estan definidas igual.\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            # En esta última parte se ordena el diccionario \"opt_dict\" de tal forma que la primera entrada sea el [w,b] donde w tiene la menor norma (min||w||)\n",
        "            # y apartir de ahí escoger el nuevo w que va ir cambiando con pasos más pequeños y así ir mejorando la escogencia de w.\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mPwayTHK41O0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQgJ5cDO3dQ"
      },
      "source": [
        "La siguiente parte corresponde a la subclase que permite realizar la predicción de nuevos datos. En particular, de aquellos que no tienen una etiqueta definida."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# En esta parte se desarrolla la predicción para un conjunto de datos de prueba. Para hacer la clasificación se mira la función sign( x.w+b )\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ANxAN0a37WIq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPDbjgLvPA25"
      },
      "source": [
        "Finalmente, el código almacena los datos de entrenamiento en un diccionario y llama a la clase \"Support_Vector_Machine\" por svm1 para entrenarlo con los datos definidos en el diccionario."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Este es el diccionario que contiene todos los datos de entrenamiento, clasificados por -1 si no tiene la caracteristica en estudio y 1 si tiene la caracteristica en estudio      \n",
        "data_dict = {-1:np.array([[1,7],\n",
        "                          [2,8],\n",
        "                          [3,8],]),\n",
        "             \n",
        "             1:np.array([[5,1],\n",
        "                         [6,-1],\n",
        "                         [7,3],])}\n",
        "\n",
        "# Aquí se llama a la clase Support_Vector_Machine() como svm1\n",
        "svm1 = Support_Vector_Machine()\n",
        "\n",
        "# Aquí se ejecuta la subclase de entrenamiento de la clase svm1 usando los datos de entrenamiento definidos en el diccionario \"data_dict\"\n",
        "svm1.fit(data_dict)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JszDIqZx7j1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación resolveremos las preguntas 5) y 6) en los distintos datasaets en donde mostraremos como la implementación del programa SVM funciona bajo ciertas restricciones y como este a su vez permite dar evidencia cuantitativa de que los resultados generalizan el problema de clasificación trabajado por la SVM."
      ],
      "metadata": {
        "id": "_T4JSjhRDW9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación para la primera base de datos (Banknote authentication Data Set)"
      ],
      "metadata": {
        "id": "TrwyQeqkYD6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder implementar el código se introdujo una tolerancia para la cual la SVM puede fallar en algunos datos. Sin embargo, como se vera a continuación esto sigue proveyendo buenos resultados."
      ],
      "metadata": {
        "id": "64sqnVc9zXLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBJM4zO-5aSk"
      },
      "outputs": [],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine_1:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        opt_dict = {}\n",
        "\n",
        "        transforms = [[1,1,1,1],\n",
        "                      [1,1,1,-1],\n",
        "                      [1,1,-1,1],\n",
        "                      [1,1,-1,-1],\n",
        "                      [1,-1,1,1],\n",
        "                      [1,-1,1,-1],\n",
        "                      [1,-1,-1,1],\n",
        "                      [-1,1,1,1],\n",
        "                      [-1,1,1,-1],\n",
        "                      [-1,1,-1,1],\n",
        "                      [-1,1,-1,-1],\n",
        "                      [-1,-1,1,1],\n",
        "                      [-1,-1,1,-1],\n",
        "                      [-1,-1,-1,1],\n",
        "                      [-1,-1,-1,-1],\n",
        "                      [1,-1,-1,-1]]\n",
        "\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "        all_data = []\n",
        "\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01,\n",
        "                      # point of expense:\n",
        "                      self.max_feature_value * 0.001]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10\n",
        "\n",
        "        tolerance = 0.03\n",
        "        total_data = self.data[1].shape[0] + self.data[-1].shape[0]\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum,latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "                        misses = 0\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    misses = misses + 1\n",
        "                                    # found_option = False\n",
        "                        \n",
        "                        rate = misses/total_data\n",
        "                        if rate > tolerance:\n",
        "                          found_option = False\n",
        "                                    \n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            print(norms)\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "\n",
        "    def predict_vector(self, X):\n",
        "        #X is a set of data, predict_vector is an array with the predictions\n",
        "        len_data = X.shape[0]\n",
        "        y = np.zeros(len_data)\n",
        "        for i in range(len_data):\n",
        "          y[i] = self.predict(X[i])\n",
        "        return np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte se importan los datos para poder crear el diccionario que entrenara la SVM. Para esto se toma una muestra aleatoria del $80 \\%$ de los datos y se toma el restante $20 \\%$ de los datos para realizar predicciones y comprobar que la SVM generaliza los datos."
      ],
      "metadata": {
        "id": "j4grSg26-Rtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", sep=\",\", header=None)\n",
        "\n",
        "percentage_sample = 0.8\n",
        "sample_size = df.shape[0]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df[[0,1,2,3]],\n",
        "    df[4],\n",
        "    test_size=0.2,\n",
        "    stratify=df[4],\n",
        "    random_state=24,\n",
        ")\n",
        "\n",
        "\n",
        "positive_class = x_train.loc[df[4] == 1]\n",
        "# positive_class = positive_class.sample(int(sample_size*percentage_sample))\n",
        "positive_class = positive_class[[0,1,2,3]].to_numpy()\n",
        "negative_class = x_train.loc[df[4] == 0]\n",
        "# negative_class = negative_class.sample(int(sample_size*percentage_sample))\n",
        "negative_class = negative_class[[0,1,2,3]].to_numpy()\n",
        "\n",
        "\n",
        "training_data_dict = {1 : positive_class,\n",
        "        -1: negative_class}"
      ],
      "metadata": {
        "id": "_BVs4NNzrrEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_1 = Support_Vector_Machine_1()\n",
        "svm_1.fit(training_data_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "dKHpnb03gGsk",
        "outputId": "99216162-6618-4c69-813d-8a7a7db2f4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "[107.56439999999984, 121.90631999999985]\n",
            "Optimized a step.\n",
            "[100.39343999999984, 107.56439999999984, 121.90631999999985]\n",
            "Optimized a step.\n",
            "[99.67634399999984, 100.39343999999984, 107.56439999999984, 121.90631999999985]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte se evalua la precisión de la SVM prediciendo los datos de prueba y así poder medir el desempeño del programa generalizando los datos."
      ],
      "metadata": {
        "id": "WJWrSY5T--B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svm_1.predict_vector(x_test.to_numpy())\n",
        "y_test1 = np.where(y_test < 1, -1, 1)\n",
        "print(classification_report(y_test1, y_pred, target_names = ['Clase Positiva (1)', 'Clase Negativa (-1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXmgg9FIiL2y",
        "outputId": "b87be6c6-f455-4132-ea04-1bd6be967141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            " Clase Positiva (1)       0.99      0.93      0.96       153\n",
            "Clase Negativa (-1)       0.92      0.98      0.95       122\n",
            "\n",
            "           accuracy                           0.96       275\n",
            "          macro avg       0.95      0.96      0.96       275\n",
            "       weighted avg       0.96      0.96      0.96       275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver como la SVM logra tener una precisión general del $95 \\% $ prediciendo los datos de prueba, mostrandonos que este data set es en un alto porcentaje linealmente separable salvo algunos outliers. Sin embargo, cabe resaltar que hay un mayor sesgo prediciendo aquellos datos de la clase negativa lo cual puede ser producto de que la frontera de decisión no esta muy bien definida. Sin embargo, el SVM logra generalizar el comportamiento de los datos al $20 \\%$ de los datos que no se utilizaron en el entrenamiento de la máquina."
      ],
      "metadata": {
        "id": "PGIaean3_RbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación para la segunda base de datos (Occupancy Detection Data Set)"
      ],
      "metadata": {
        "id": "KcAMVM6CYdRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder implementar el código se introdujo una tolerancia para la cual la SVM puede fallar en algunos datos. Sin embargo, como se vera a continuación esto sigue proveyendo buenos resultados."
      ],
      "metadata": {
        "id": "_otepJwI8Aep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3vgCTrZo8hm"
      },
      "outputs": [],
      "source": [
        "# https://pythonprogramming.net/svm-optimization-python-2-machine-learning-tutorial/?completed=/svm-optimization-python-machine-learning-tutorial/\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests, zipfile, io\n",
        "\n",
        "style.use('ggplot')\n",
        "\n",
        "class Support_Vector_Machine_2:\n",
        "    def __init__(self, visualization=True):\n",
        "        self.visualization = visualization\n",
        "        self.colors = {1:'r',-1:'b'}\n",
        "        if self.visualization:\n",
        "            self.fig = plt.figure()\n",
        "            self.ax = self.fig.add_subplot(1,1,1)\n",
        "    # train\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        # { ||w||: [w,b] }\n",
        "        opt_dict = {}\n",
        "\n",
        "        transforms = [[1,1,1,1,1],\n",
        "                      [1,1,1,1,-1],\n",
        "                      [1,1,1,-1,1],\n",
        "                      [1,1,1,-1,-1],\n",
        "                      [1,1,-1,1,1],\n",
        "                      [1,1,-1,1,-1],\n",
        "                      [1,1,-1,-1,1],\n",
        "                      [1,1,-1,-1,-1],\n",
        "                      [1,-1,1,1,1],\n",
        "                      [1,-1,1,1,-1],\n",
        "                      [1,-1,1,-1,1],\n",
        "                      [1,-1,1,-1,-1],\n",
        "                      [1,-1,-1,1,1],\n",
        "                      [1,-1,-1,1,-1],\n",
        "                      [1,-1,-1,-1,1],\n",
        "                      [-1,1,1,1,1],\n",
        "                      [-1,1,1,1,-1],\n",
        "                      [-1,1,1,-1,1],\n",
        "                      [-1,1,1,-1,-1],\n",
        "                      [-1,1,-1,1,1],\n",
        "                      [-1,1,-1,1,-1],\n",
        "                      [-1,1,-1,-1,1],\n",
        "                      [-1,1,-1,-1,-1],\n",
        "                      [-1,-1,1,1,1],\n",
        "                      [-1,-1,1,1,-1],\n",
        "                      [-1,-1,1,-1,1],\n",
        "                      [-1,-1,1,-1,-1],\n",
        "                      [-1,-1,-1,1,1],\n",
        "                      [-1,-1,-1,1,-1],\n",
        "                      [-1,-1,-1,-1,1],\n",
        "                      [-1,-1,-1,-1,-1],\n",
        "                      [1,-1,-1,-1,-1]]\n",
        "\n",
        "        all_data = []\n",
        "        for yi in self.data:\n",
        "            for featureset in self.data[yi]:\n",
        "                for feature in featureset:\n",
        "                    all_data.append(feature)\n",
        "\n",
        "        self.max_feature_value = max(all_data)\n",
        "        self.min_feature_value = min(all_data)\n",
        "        all_data = None\n",
        "\n",
        "        # support vectors yi(xi.w+b) = 1\n",
        "        \n",
        "\n",
        "        step_sizes = [self.max_feature_value * 0.1,\n",
        "                      self.max_feature_value * 0.01]\n",
        "                      # point of expense:\n",
        "                      # self.max_feature_value * 0.001,]\n",
        "\n",
        "        \n",
        "        \n",
        "        # extremely expensive\n",
        "        b_range_multiple = 5\n",
        "        # we dont need to take as small of steps\n",
        "        # with b as we do w\n",
        "        b_multiple = 5\n",
        "        latest_optimum = self.max_feature_value*10\n",
        "\n",
        "        tolerance = 0.2\n",
        "        total_data = self.data[1].shape[0] + self.data[-1].shape[0]\n",
        "\n",
        "        for step in step_sizes:\n",
        "            w = np.array([latest_optimum,latest_optimum,latest_optimum,latest_optimum,latest_optimum])\n",
        "            # we can do this because convex\n",
        "            optimized = False\n",
        "            while not optimized:\n",
        "                for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
        "                                   self.max_feature_value*b_range_multiple,\n",
        "                                   step*b_multiple):\n",
        "                    for transformation in transforms:\n",
        "                        w_t = w*transformation\n",
        "                        found_option = True\n",
        "                        # weakest link in the SVM fundamentally\n",
        "                        # SMO attempts to fix this a bit\n",
        "                        # yi(xi.w+b) >= 1\n",
        "                        # \n",
        "                        # #### add a break here later..\n",
        "\n",
        "                        misses = 0\n",
        "                        for i in self.data:\n",
        "                            for xi in self.data[i]:\n",
        "                                yi=i\n",
        "                                # Verifiy constraints\n",
        "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
        "                                    misses = misses + 1\n",
        "                                    rate = misses/total_data\n",
        "                                    if rate > tolerance:\n",
        "                                        found_option = False\n",
        "                                        break\n",
        "                                      \n",
        "                                    # found_option = False\n",
        "                        \n",
        "                        # rate = misses/total_data\n",
        "                        # if rate > tolerance:\n",
        "                          # found_option = False\n",
        "\n",
        "                        if found_option:\n",
        "                            # Computes norm\n",
        "                            opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
        "\n",
        "                if w[0] < 0:\n",
        "                    optimized = True\n",
        "                    print('Optimized a step.')\n",
        "                else:\n",
        "                    w = w - step\n",
        "\n",
        "            norms = sorted([n for n in opt_dict])\n",
        "            #||w|| : [w,b]\n",
        "            print(norms)\n",
        "            opt_choice = opt_dict[norms[0]]\n",
        "            self.w = opt_choice[0]\n",
        "            self.b = opt_choice[1]\n",
        "            latest_optimum = opt_choice[0][0]+step*2\n",
        "            \n",
        "\n",
        "    def predict(self,features):\n",
        "        # sign( x.w+b )\n",
        "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
        "        return classification\n",
        "    \n",
        "    def predict_vector(self, X):\n",
        "        #X is a set of data, predict_vector is an array with the predictions\n",
        "        len_data = X.shape[0]\n",
        "        y = np.zeros(len_data)\n",
        "        for i in range(len_data):\n",
        "          y[i] = self.predict(X[i])\n",
        "        return np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se extraen los distintos archivos de la carpeta Zip del data set de \"Detección de ocupación\". Inicialmente se extrae el archivo \"datatraining.txt\" para poder formar el diccionario que tendra los datos de entrenamiento."
      ],
      "metadata": {
        "id": "uMm5jKRm2BSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "f1 = z.open(\"datatraining.txt\")\n",
        "df1 = pd.read_csv(f1, sep=\",\")\n",
        "\n",
        "percentage_sample = 0.01\n",
        "sample_size = df1.shape[0]\n",
        "\n",
        "positive = df1.loc[df1[\"Occupancy\"] == 1]\n",
        "negative = df1.loc[df1[\"Occupancy\"] == 0]\n",
        "\n",
        "training_positive = positive.iloc[:, 1:6]\n",
        "# positive_arr = training_positive.to_numpy()\n",
        "\n",
        "training_positive_2 = training_positive.sample(int(training_positive.shape[0]*percentage_sample))\n",
        "positive_arr_2 = training_positive_2.to_numpy()\n",
        "\n",
        "training_negative = negative.iloc[:, 1:6]\n",
        "# negative_arr = training_negative.to_numpy()\n",
        "\n",
        "training_negative_2 = training_negative.sample(int(training_negative.shape[0]*percentage_sample))\n",
        "negative_arr_2 = training_negative_2.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "training_data_dict_2 = {-1:negative_arr_2,\n",
        "                      1:positive_arr_2}"
      ],
      "metadata": {
        "id": "l7zq_Osiqpju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_2 = Support_Vector_Machine_2()\n",
        "svm_2.fit(training_data_dict_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "ttikPJZ7seOd",
        "outputId": "5c3e15f3-d7b4-475a-953c-584045e424c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized a step.\n",
            "[438.7724388848927, 438.7724388848998, 877.5448777697961, 1316.3173166546924, 1755.0897555395886]\n",
            "Optimized a step.\n",
            "[43.877243888486454, 87.75448777697608, 131.6317316654657, 175.50897555395534, 219.38621944244497, 263.2634633309346, 307.1407072194242, 351.01795110791386, 394.89519499640346, 438.7724388848927, 438.7724388848931, 438.7724388848998, 482.6496827733827, 526.5269266618724, 570.404170550362, 614.2814144388516, 658.1586583273413, 702.0359022158309, 745.9131461043205, 789.7903899928101, 833.6676338812997, 877.5448777697894, 877.5448777697961, 921.422121658279, 965.2993655467687, 1009.1766094352582, 1053.053853323748, 1096.9310972122375, 1140.8083411007271, 1184.6855849892167, 1228.5628288777064, 1272.4400727661957, 1316.3173166546853, 1316.3173166546924, 1755.0897555395886]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la parte siguiente se exportan los archivos que se van a utilizar para hacer las predicciones y poder medir posteriormente el grado de precisión del SVM para este data set de \"Detección de ocupación\"."
      ],
      "metadata": {
        "id": "Ee-3ovV_2wHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f2 = z.open(\"datatest.txt\")\n",
        "df2 = pd.read_csv(f2, sep =',')\n",
        "\n",
        "f3 = z.open(\"datatest2.txt\")\n",
        "df3 = pd.read_csv(f3, sep =',')\n",
        "\n",
        "df4 = pd.concat([df2, df3])\n",
        "\n",
        "positive = df4.loc[df4[\"Occupancy\"] == 1]\n",
        "negative = df4.loc[df4[\"Occupancy\"] == 0]\n",
        "\n",
        "predict_positive = positive.iloc[:, 1:6]\n",
        "positive_arr = predict_positive.to_numpy()\n",
        "\n",
        "predict_negative = negative.iloc[:, 1:6]\n",
        "negative_arr = predict_negative.to_numpy()\n",
        "\n",
        "predict = df4.iloc[:, 1:6]\n",
        "predict_array = predict.to_numpy()\n",
        "\n",
        "y_test = df4.iloc[:,6]\n",
        "y_test_arr = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "irbvSpgIuVj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y_pred2 = svm_2.predict_vector(predict_array)\n",
        "y_test2 = np.where(y_test_arr < 1, -1, 1)\n",
        "print(classification_report(y_test2, y_pred2, target_names = ['Clase Ocupado (1)', 'Clase No Ocupado (-1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL5G1Y-Rsqr1",
        "outputId": "7186c8b4-45b6-4423-d935-d49f81790c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Clase Ocupado (1)       0.84      0.97      0.90      9396\n",
            "Clase No Ocupado (-1)       0.82      0.41      0.54      3021\n",
            "\n",
            "             accuracy                           0.83     12417\n",
            "            macro avg       0.83      0.69      0.72     12417\n",
            "         weighted avg       0.83      0.83      0.81     12417\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver como la SVM logra tener una precisión general del $83 \\% $ prediciendo los datos de prueba, mostrandonos que este data set es en un alto porcentaje linealmente separable salvo algunos outliers. Sin embargo, cabe resaltar que hay un mayor sesgo prediciendo aquellos datos de la clase negativa lo cual puede ser producto de que la frontera de decisión no esta muy bien definida. Aquí hay que hacer la salvedad de que esta precisión corresponde a tan solo tomar el $1 \\%$ de los datos en \"datatraining.txt\" y usarlos para entrenar el SVM. Sin embargo, a pesar de la baja cantidad de datos en el entrenamiento podemos apreciar un gran desempeño del programa prediciendo los nuevos datos."
      ],
      "metadata": {
        "id": "JzXi3iMqD3qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otra parte, si utilizamos la libreria scikit-learn para entrenar estos datos con la implementación del SVM de dicha libreria podemos observar que la separación de los datos por un hiperplano tiene un gran porcentaje de precisión, como se puede observar a continuación:"
      ],
      "metadata": {
        "id": "Zp_eSC-M2gbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "X = df1.iloc[:, 1:6].to_numpy()\n",
        "y = df1.iloc[:, 6].to_numpy()\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "H5EYp8eXy-6u",
        "outputId": "0cfe8598-a4b0-42ca-cafa-fabb66aa708b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(predict_array)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcGkET-dzXyk",
        "outputId": "aa17b00a-d6e2-4184-beb9-c040371d9320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test_arr, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPyPBVsu00JV",
        "outputId": "4b966224-0ab8-4048-a43f-e34482409476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.989530482403157\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}